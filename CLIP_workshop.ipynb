{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CLIP workshop - Image classification with Natural Language Supervision**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "author: Micha≈Ç Gozdera \n",
        "\n",
        "email: gozdera.michal@gmail.com\n",
        "\n",
        "LinkedIn: https://linkedin.com/in/gozderam\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This workshop is based on:\n",
        "\n",
        "*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever Proceedings of the 38th International Conference on Machine Learning, PMLR 139:8748-8763, 2021*\n",
        "\n",
        "CLIP paper: http://proceedings.mlr.press/v139/radford21a.html\n",
        "\n",
        "CLIP github page: https://github.com/openai/CLIP"
      ],
      "metadata": {
        "id": "WFN6gWcW1ams"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1:**\n",
        "Classify an image with CLIP and experiment with Natural Language Supervision. The goal is to classify an image of a cat and investigate how CLIP works with different class labels/descriptions. "
      ],
      "metadata": {
        "id": "g0fqNzbqnvTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial setups"
      ],
      "metadata": {
        "id": "dfrriBQvezBm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3bp2tySm0jY"
      },
      "outputs": [],
      "source": [
        "# install required packages\n",
        "!pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import torch\n",
        "import clip\n",
        "from IPython.display import Image, display\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "6sP75eg4yi0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decide whether we use CPU or GPU (for this task CPU is enough, so you can leave the default settings of Google Colab)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "WfnOoDp8_OfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions definitions"
      ],
      "metadata": {
        "id": "O7CQoUl4e-BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image(url, img_name):\n",
        "  \"\"\"\n",
        "  Downloads image from url and saves it into a file with name img_name. Returns path to the saved image.\n",
        "  \"\"\"\n",
        "  img_ext = url[-4:]\n",
        "  img_path = img_name + img_ext\n",
        "\n",
        "  img_data = requests.get(url).content\n",
        "\n",
        "  with open(img_path, 'wb') as handler:\n",
        "    handler.write(img_data)\n",
        "\n",
        "  return img_path\n"
      ],
      "metadata": {
        "id": "Gl3ORLBsr4NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_label_probs(probs, class_labels):\n",
        "  \"\"\"\n",
        "  Prints label probabilities (probs) in a readable form. Argument class_labels represents the class labels/descriptions.\n",
        "  \"\"\"\n",
        "  pred_label_idx = np.argmax(probs[0])\n",
        "\n",
        "  print(\"Label probs:\") \n",
        "  for i, descr in enumerate(class_labels):\n",
        "    if i == pred_label_idx:\n",
        "      print(\"\\033[1m\" + f\"{descr}: \" + '{0:.5f}'.format(probs[0][i]) + \"\\033[0m\")\n",
        "    else:\n",
        "      print(f\"{descr}: \" + '{0:.5f}'.format(probs[0][i]))"
      ],
      "metadata": {
        "id": "TxnQWgnAw_bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_CLIP_prediction(model, preprocess, img_path, text_descrs):\n",
        "  \"\"\"\n",
        "  Performs CLIP prediction with a given model and image preprocess. An image stored in img_path is classified. Argument text_descrs represents the class labels/descriptions. \n",
        "\n",
        "  Returns the CLIP probabilities of an image belonging to each class.\n",
        "  \"\"\"\n",
        "  image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "  text = clip.tokenize(text_descrs).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "  return probs\n"
      ],
      "metadata": {
        "id": "RNxZLUWFxYFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time to experiment with CLIP and NLP supervision!"
      ],
      "metadata": {
        "id": "_tbVt4JKyozX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download CLIP model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "7OT_h-C_yqX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get an image of a cat from Internet\n",
        "img_path = get_image(\"https://s.marketwatch.com/public/resources/images/MW-BM546_pfcats_MG_20131004154326.jpg\", \"cat\")\n",
        "\n",
        "# view the image\n",
        "display(Image.open(img_path))"
      ],
      "metadata": {
        "id": "nVyQ9yqSzAFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with something easy - only one class makes any sense:"
      ],
      "metadata": {
        "id": "goi0FqlOfiZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible class labels/class descriptions\n",
        "text_descrs = [\"an oven\", \"a university\", \"a cat\"]\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)"
      ],
      "metadata": {
        "id": "QcPTwYJ02okv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's move to more reasonable classes, only the animals:"
      ],
      "metadata": {
        "id": "XF-ksWbxfpOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible class labels/class descriptions\n",
        "text_descrs = [\"a horse\", \"a dog\", \"a cat\"]\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)\n"
      ],
      "metadata": {
        "id": "49zMsZNFnxQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we include a more general concept, like *an animal*?"
      ],
      "metadata": {
        "id": "Y0K-c1Bhfuir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible class labels/class descriptions\n",
        "text_descrs = [\"a horse\", \"a dog\", \"a cat\", \"an animal\"]\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)"
      ],
      "metadata": {
        "id": "ajqqEP7XtxLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the cat has a lot of money. Try to describe the picture in more detail. Instead of just a class label, let's put some adjectives describing the cat."
      ],
      "metadata": {
        "id": "Cb31fjI1ge68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible class labels/class descriptions\n",
        "text_descrs = [\"a horse\", \"a dog\", \"a cat\", \"an animal\", \"a rich cat\", \"a poor cat\"]\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)"
      ],
      "metadata": {
        "id": "z7SSD_vK2MDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How CLIP works with entire sentences instead of labels? Does it handle negative sentences? "
      ],
      "metadata": {
        "id": "1nuEEcuUgv44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible class labels/class descriptions\n",
        "text_descrs = [\"This is a cat.\", \"This is not a cat.\", \"Perhaps it is a cat.\"]\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)"
      ],
      "metadata": {
        "id": "9MLeKVOz1cgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the picture includes not only a cat. We can also see the money and caviar. Which one will be chosen by CLIP? Maybe all of them?"
      ],
      "metadata": {
        "id": "jPGEj4oehA0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible class labels/class descriptions\n",
        "text_descrs = [\"This is not only a cat, but also money and caviar.\", \n",
        "               \"This is only a cat.\", \n",
        "               \"Only money is visible in the picture.\",\n",
        "               \"Money and caviar are there, but there is no cat at all.\"]\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)"
      ],
      "metadata": {
        "id": "LKo6N04n3QR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, force CLIP to choose a label from a set of labels that are all correct. Compare the probability values with f.e., the first case, where\n",
        "`text_descrs = [\"an oven\", \"a university\", \"a cat\"]`"
      ],
      "metadata": {
        "id": "2kaQjLxhhYWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# possible class labels/class descriptions\n",
        "text_descrs = [\"money\", \n",
        "               \"a cat\", \n",
        "               \"a caviar\"]\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)"
      ],
      "metadata": {
        "id": "r23lhTCd5Mlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TODO**: Now it is your turn to challenge the CLIP! Try classifying your own picture of anything (your favorite pet, food, building, ...). Investigate  different NLP supervision variants. "
      ],
      "metadata": {
        "id": "3uEABsyw43Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: paste some URL from the Internet (first argument) and the filename (second argument)\n",
        "img_path = get_image(\"\", \"\")\n",
        "\n",
        "# view the image\n",
        "display(Image.open(img_path))"
      ],
      "metadata": {
        "id": "hbn6iIOz1UAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: experiment with various possible labels (put them as strings in the array)\n",
        "text_descrs = []\n",
        "\n",
        "# perform CLIP classification\n",
        "probs = do_CLIP_prediction(model, preprocess, img_path, text_descrs)\n",
        "\n",
        "# print CLIP classification results\n",
        "print_label_probs(probs, text_descrs)"
      ],
      "metadata": {
        "id": "M443gXj65IgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2:**\n",
        "Examine the zero-shot prediction for CLIP. We will be working with the CIFAR10 dataset and will try to classify its test images with CLIP without any training! \n",
        "\n",
        "**NOTE:** For this task, change the Google Colab environment type to GPU to speed up the calculations."
      ],
      "metadata": {
        "id": "Zn1GylxK5xYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial setup"
      ],
      "metadata": {
        "id": "mDbK33ICEGFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "bdRe5HRwDplg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2suRNTwm_2do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "R7GJD1Dr-_KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions definitions"
      ],
      "metadata": {
        "id": "IbX1DokjkGG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(preprocess):\n",
        "  \"\"\"\n",
        "  Loads the CIFAR10 dataset. \n",
        "  \n",
        "  Returns train and test parts.\n",
        "  \"\"\"\n",
        "  root = os.path.expanduser(\"/content\")\n",
        "  train = CIFAR10(root, download=True, train=True, transform=preprocess)\n",
        "  test = CIFAR10(root, download=True, train=False, transform=preprocess)\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "FptqRfSgAEBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_CLIP(dataset, class_labels):\n",
        "  \"\"\"\n",
        "  Performs CLIP predictions for dataset images. Argument class_labels represents the class labels/descriptions.\n",
        "\n",
        "  Returns the true label indices and predicted label indices.\n",
        "  \"\"\"\n",
        "\n",
        "  y = []\n",
        "  y_hat = []\n",
        "\n",
        "  # extract all labels present in the dataset and transform them to labels\n",
        "  all_texts = torch.cat([clip.tokenize(c) for c in class_labels]).to(device)\n",
        "  with torch.no_grad():\n",
        "    for images, labels in tqdm(DataLoader(dataset, batch_size=500)):\n",
        "      logits_per_image, logits_per_text = model(images.to(device), all_texts)\n",
        "      probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "      y.append(labels.numpy())\n",
        "      y_hat.append(np.argmax(probs, axis=1))\n",
        "    \n",
        "  return np.concatenate(y), np.concatenate(y_hat)"
      ],
      "metadata": {
        "id": "mTrTsjK4ASTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(y, y_hat):\n",
        "  \"\"\"\n",
        "  Calculate and prints the accuracy. y represents true label indices and y_hat - predicted label indices. \n",
        "  \"\"\"\n",
        "  acc = np.mean(y == y_hat)\n",
        "  print(f'Accuracy on testset: ' + '{0:.2f}'.format(acc*100)+'%')"
      ],
      "metadata": {
        "id": "aVCPGDy7NzaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y, y_hat, class_labels, annotations = True):\n",
        "  \"\"\"\n",
        "  Plots confusion matrix. y represents true label indices and y_hat - predicted label indices. If annotations is set to True, the confusion matrix will include numeric values. \n",
        "  \"\"\"\n",
        "  cf_matrix = confusion_matrix(y, y_hat)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10,8))\n",
        "  sns.heatmap(cf_matrix, annot=annotations, cmap='Blues', ax = ax, fmt='g')\n",
        "\n",
        "  ax.set_xlabel('\\nPredicted Values')\n",
        "  ax.set_ylabel('Actual Values ')\n",
        "\n",
        "  ax.xaxis.set_ticklabels(class_labels, rotation='vertical')\n",
        "  ax.yaxis.set_ticklabels(class_labels, rotation='horizontal')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "mVaGajf3N2SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform zero-shot CLIP classification for CIFAR10"
      ],
      "metadata": {
        "id": "3ETpJ5bklsDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model and dataset\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "_, testset = load_data(preprocess)"
      ],
      "metadata": {
        "id": "YQXoWeR7Jdlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract class labels \n",
        "class_labels = [f\"a {c}\" for c in testset.classes]\n",
        "\n",
        "# predict class labels for the testset\n",
        "y, y_hat = test_CLIP(testset, class_labels)"
      ],
      "metadata": {
        "id": "w6LDJjVJAwVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Investigate the results of zero-shot CLIP classification\n",
        "Print the accuracy obtained on the testset and the confusion matrix."
      ],
      "metadata": {
        "id": "PbKEnv1PGrx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy on the testset\n",
        "calc_accuracy(y, y_hat)"
      ],
      "metadata": {
        "id": "KmUZ8Eq3GtiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(y, y_hat, class_labels)"
      ],
      "metadata": {
        "id": "Z5D9rtylGrPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TODO:** Experiment with different class labels and again check how different variants of NLP supervision influence CLIP accuracy! Check the accuracy and confusion matrix for each variant.\n",
        "\n",
        "You can, for example, replace labels with whole sentences, like: \n",
        "\n",
        "*cat* --> *This is a picture of a cat.*\n",
        "\n",
        "or add some adjectives to the nouns."
      ],
      "metadata": {
        "id": "B3drdOE-JHuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: extract class labels \n",
        "class_labels = # ...\n",
        "\n",
        "# predict class labels for the testset\n",
        "y, y_hat = test_CLIP(testset, class_labels)"
      ],
      "metadata": {
        "id": "gQSd7RtlJPsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy on the testset\n",
        "calc_accuracy(y, y_hat)"
      ],
      "metadata": {
        "id": "bd9BQPupJusZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(y, y_hat, class_labels)"
      ],
      "metadata": {
        "id": "mfxQsx-9J0Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TODO:** Try another dataset, for example CIFAR100. \n",
        "CIFAR10 contains only 10 classes and CIFAR100 - 100. Comapare the zero-shot CLIP accuracy for CIFAR10 and CIFAR100."
      ],
      "metadata": {
        "id": "6LSWK69FQISE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "def load_data_cifar100(preprocess):\n",
        "  root = os.path.expanduser(\"/content\")\n",
        "  train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "  test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "PrAbslM4P78u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, testset = load_data_cifar100(preprocess)"
      ],
      "metadata": {
        "id": "kkR288MGQV_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = [f\"a {c}\" for c in testset.classes]\n",
        "\n",
        "y, y_hat = test_CLIP(testset, class_labels)\n",
        "\n",
        "calc_accuracy(y, y_hat)\n",
        "plot_confusion_matrix(y, y_hat, class_labels, annotations=False)"
      ],
      "metadata": {
        "id": "khSHCdwhQjnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**\n",
        "### **TODO**: Use CLIP as a feature extractor. On the extracted features, use a simple classifier, like logistic regression, SVM, or kNN. Compare results with CLIP without supervision. "
      ],
      "metadata": {
        "id": "yhy6g9QFR1m7"
      }
    }
  ]
}